{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc22ec43",
   "metadata": {},
   "source": [
    "# Notebook 02 â€” Chroma indexing + persistence (LlamaIndex + OpenAI embeddings)\n",
    "\n",
    "This notebook loads the existing deterministic notes dataset, chunks it into nodes, and then **builds or loads** a persisted Chroma index using OpenAI embeddings.\n",
    "\n",
    "It only validates indexing + retrieval behavior (no generation/answer synthesis yet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e0581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "- PROJECT_ROOT: C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic-rag-second-brain\n",
      "- EMBED_MODEL: text-embedding-3-small\n",
      "- CHROMA_DIR: C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic-rag-second-brain\\data\\processed\\chroma\n",
      "- RESET_INDEX: False\n",
      "- TOP_K: 6\n",
      "- OPENAI_API_KEY set: yes\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def _find_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for base in (cwd, *cwd.parents):\n",
    "        if (base / \"src\" / \"config.py\").exists():\n",
    "            return base\n",
    "        nested = base / \"agentic-rag-second-brain\"\n",
    "        if (nested / \"src\" / \"config.py\").exists():\n",
    "            return nested\n",
    "    raise RuntimeError(\"Could not locate project root containing src/config.py\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "from src.config import settings\n",
    "from src.dataset import ensure_dataset_exists\n",
    "from src.ingestion import chunk_documents, load_markdown_documents\n",
    "from src.index_store import build_or_load_index\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", settings.embed_model)\n",
    "CHROMA_DIR = Path(os.getenv(\"CHROMA_DIR\", settings.chroma_dir)).resolve()\n",
    "RESET_INDEX = os.getenv(\"RESET_INDEX\", settings.reset_index).strip() == \"1\"\n",
    "TOP_K = int(os.getenv(\"TOP_K\", settings.top_k))\n",
    "\n",
    "print(\"Config:\")\n",
    "print(f\"- PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"- EMBED_MODEL: {EMBED_MODEL}\")\n",
    "print(f\"- CHROMA_DIR: {CHROMA_DIR}\")\n",
    "print(f\"- RESET_INDEX: {RESET_INDEX}\")\n",
    "print(f\"- TOP_K: {TOP_K}\")\n",
    "print(f\"- OPENAI_API_KEY set: {'yes' if OPENAI_API_KEY else 'no'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4da11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY is required for Notebook 02. \"\n",
    "        \"Set it before running, for example: `export OPENAI_API_KEY='your-key'`.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ed4c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notes_dir': 'data\\\\raw\\\\notes', 'num_notes': 14, 'filenames': ['2025-01-10-embedding-model-cost-first.md', '2025-02-02-chunking-large-windows.md', '2025-03-07-meeting-search-quality.md', '2025-03-18-embedding-evaluation-q1.md', '2025-04-14-chunking-feedback.md', '2025-05-22-research-hybrid-retrieval.md', '2025-06-30-meeting-onboarding-notes.md', '2025-07-05-embedding-model-quality-shift.md', '2025-08-15-research-metadata-schema.md', '2025-09-03-chunking-small-overlap.md', '2025-10-02-meeting-demo-retro.md', '2025-10-21-embedding-rollout-postmortem.md', '2025-11-12-chunking-maintenance.md', '2025-12-01-roadmap-notes-q1.md'], 'created_or_updated': [], 'force_rebuild': False}\n"
     ]
    }
   ],
   "source": [
    "dataset_summary = ensure_dataset_exists(force_rebuild=False)\n",
    "print(dataset_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55365b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: 14\n",
      "Chunked nodes: 14\n",
      "Sample metadata keys: ['chunk_id', 'doc_date', 'doc_id', 'doc_title', 'source_path', 'tags']\n"
     ]
    }
   ],
   "source": [
    "notes_dir = PROJECT_ROOT / \"data\" / \"raw\" / \"notes\"\n",
    "documents = load_markdown_documents(notes_dir)\n",
    "nodes = chunk_documents(documents)\n",
    "\n",
    "print(f\"Loaded documents: {len(documents)}\")\n",
    "print(f\"Chunked nodes: {len(nodes)}\")\n",
    "if nodes:\n",
    "    print(\"Sample metadata keys:\", sorted(nodes[0].metadata.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5067695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: notes\n",
      "Built this run: False\n",
      "Persist dir: C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic-rag-second-brain\\data\\processed\\chroma\n",
      "Vector count: 14\n"
     ]
    }
   ],
   "source": [
    "index_info = build_or_load_index(\n",
    "    nodes=nodes,\n",
    "    reset=RESET_INDEX,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    embed_model=EMBED_MODEL,\n",
    ")\n",
    "\n",
    "index = index_info[\"index\"]\n",
    "print(f\"Collection: {index_info['collection_name']}\")\n",
    "print(f\"Built this run: {index_info['built']}\")\n",
    "print(f\"Persist dir: {index_info['chroma_dir']}\")\n",
    "print(f\"Vector count: {index_info['vector_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d92175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>doc_date</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source_path</th>\n",
       "      <th>text_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.420195</td>\n",
       "      <td>2025-07-05</td>\n",
       "      <td>Embedding Model Decision Update: Quality Priority</td>\n",
       "      <td>1843528f9966ef38e563dc60ec056795eab0a0b1:7</td>\n",
       "      <td>C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...</td>\n",
       "      <td>Query logs now show many semantically subtle q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.399181</td>\n",
       "      <td>2025-01-10</td>\n",
       "      <td>Embedding Model Decision: Cost-First Default</td>\n",
       "      <td>627528ea8b8af5f59df5fae9b902a22869e8e53f:0</td>\n",
       "      <td>C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...</td>\n",
       "      <td>We should standardize on EmbedLite-v1 for now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389766</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>Q1 Embedding Evaluation Notes</td>\n",
       "      <td>aa23b701925fb16b6312fa7dc6f53474541c46fc:3</td>\n",
       "      <td>C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...</td>\n",
       "      <td>Compared EmbedLite-v1 and EmbedPro-v2 on histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.345105</td>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>Embedding Rollout Postmortem</td>\n",
       "      <td>3ce7ccdc6cae9be14a952f15d541a4f87c73ec51:11</td>\n",
       "      <td>C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...</td>\n",
       "      <td>After switching to EmbedPro-v2, retrieval qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.273567</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>Demo Retro: Internal Stakeholder Session</td>\n",
       "      <td>23d249bcab98910ef4133cf9ece7801e2b179a31:10</td>\n",
       "      <td>C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...</td>\n",
       "      <td>Stakeholders responded positively to timeline-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.272027</td>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>Research Snippet: Hybrid Retrieval</td>\n",
       "      <td>79c9b8fbd7b9030b5f342d8f45587b0bb8c4a3fb:5</td>\n",
       "      <td>C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...</td>\n",
       "      <td>A short literature scan suggests dense+sparse ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score    doc_date                                          doc_title  \\\n",
       "0  0.420195  2025-07-05  Embedding Model Decision Update: Quality Priority   \n",
       "1  0.399181  2025-01-10       Embedding Model Decision: Cost-First Default   \n",
       "2  0.389766  2025-03-18                      Q1 Embedding Evaluation Notes   \n",
       "3  0.345105  2025-10-21                       Embedding Rollout Postmortem   \n",
       "4  0.273567  2025-10-02           Demo Retro: Internal Stakeholder Session   \n",
       "5  0.272027  2025-05-22                 Research Snippet: Hybrid Retrieval   \n",
       "\n",
       "                                      chunk_id  \\\n",
       "0   1843528f9966ef38e563dc60ec056795eab0a0b1:7   \n",
       "1   627528ea8b8af5f59df5fae9b902a22869e8e53f:0   \n",
       "2   aa23b701925fb16b6312fa7dc6f53474541c46fc:3   \n",
       "3  3ce7ccdc6cae9be14a952f15d541a4f87c73ec51:11   \n",
       "4  23d249bcab98910ef4133cf9ece7801e2b179a31:10   \n",
       "5   79c9b8fbd7b9030b5f342d8f45587b0bb8c4a3fb:5   \n",
       "\n",
       "                                         source_path  \\\n",
       "0  C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...   \n",
       "1  C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...   \n",
       "2  C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...   \n",
       "3  C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...   \n",
       "4  C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...   \n",
       "5  C:\\Repos\\Intro-to-RAG-Agentic-RAG-2602\\agentic...   \n",
       "\n",
       "                                        text_preview  \n",
       "0  Query logs now show many semantically subtle q...  \n",
       "1  We should standardize on EmbedLite-v1 for now ...  \n",
       "2  Compared EmbedLite-v1 and EmbedPro-v2 on histo...  \n",
       "3  After switching to EmbedPro-v2, retrieval qual...  \n",
       "4  Stakeholders responded positively to timeline-...  \n",
       "5  A short literature scan suggests dense+sparse ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What embedding model is the most recent recommendation?\"\n",
    "retriever = index.as_retriever(similarity_top_k=TOP_K)\n",
    "results = retriever.retrieve(query)\n",
    "\n",
    "rows = []\n",
    "for result in results:\n",
    "    node = result.node\n",
    "    rows.append(\n",
    "        {\n",
    "            \"score\": float(result.score) if result.score is not None else None,\n",
    "            \"doc_date\": node.metadata.get(\"doc_date\", \"\"),\n",
    "            \"doc_title\": node.metadata.get(\"doc_title\", \"\"),\n",
    "            \"chunk_id\": node.metadata.get(\"chunk_id\", \"\"),\n",
    "            \"source_path\": node.metadata.get(\"source_path\", \"\"),\n",
    "            \"text_preview\": node.get_content()[:200].replace(\"\\n\", \" \"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ebd24c",
   "metadata": {},
   "source": [
    "### Rebuild behavior for repeated demos\n",
    "\n",
    "- Default behavior (`RESET_INDEX=0`) loads the existing persisted Chroma index if present.\n",
    "- Set `RESET_INDEX=1` to wipe `CHROMA_DIR` and fully rebuild vectors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

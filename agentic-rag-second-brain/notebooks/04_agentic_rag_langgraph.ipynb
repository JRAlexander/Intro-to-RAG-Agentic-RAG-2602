{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentic RAG with LangGraph (timeframe drift fix)\n",
        "\n",
        "This notebook adds explicit control flow to fix timeframe drift: **rewrite \u2192 retrieve \u2192 grade \u2192 bounded retry \u2192 generate with citations + confidence**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from src.config import settings\n",
        "from src.graph import build_agentic_rag_graph, run_agentic_rag\n",
        "from src.rag_baseline import baseline_rag_answer\n",
        "from src.retrieval import load_persisted_index\n",
        "\n",
        "load_dotenv()\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise EnvironmentError('OPENAI_API_KEY is missing. Set it in your environment or .env file.')\n",
        "\n",
        "print('Config loaded.')\n",
        "print(f\"OPENAI_MODEL={settings.openai_model}\")\n",
        "print(f\"TOP_K={settings.top_k}, MAX_RETRIES={settings.max_retries}, RECENCY_DAYS={settings.recency_days}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_dir = Path(settings.chroma_dir)\n",
        "if not index_dir.exists() or not any(index_dir.iterdir()):\n",
        "    raise FileNotFoundError(\n",
        "        f'Persisted index not found at {index_dir}. Run notebooks/02_indexing_chroma_llamaindex.ipynb first.'\n",
        "    )\n",
        "\n",
        "index = load_persisted_index(chroma_dir=index_dir, embed_model=settings.embed_model)\n",
        "print(f'Index loaded from {index_dir}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DRIFT_QUERY = 'What embedding model should we use?'\n",
        "\n",
        "# Optional reminder of baseline behavior from Notebook 03\n",
        "baseline = baseline_rag_answer(\n",
        "    index=index,\n",
        "    query=DRIFT_QUERY,\n",
        "    top_k=int(settings.top_k),\n",
        "    model=settings.openai_model,\n",
        "    temperature=float(settings.temperature),\n",
        "    max_context_chars=int(settings.max_context_chars),\n",
        ")\n",
        "print('Baseline answer (Notebook 03 style):')\n",
        "print(baseline['answer'])\n",
        "print('Baseline citations:')\n",
        "for c in baseline['citations']:\n",
        "    print('-', c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = build_agentic_rag_graph(\n",
        "    index=index,\n",
        "    openai_model=settings.openai_model,\n",
        "    temperature=float(settings.temperature),\n",
        "    top_k=int(settings.top_k),\n",
        "    max_context_chars=int(settings.max_context_chars),\n",
        "    max_retries=int(settings.max_retries),\n",
        "    recency_days=int(settings.recency_days),\n",
        "    evidence_min_recent_chunks=int(settings.evidence_min_recent_chunks),\n",
        "    use_llm_grader=settings.use_llm_grader == '1',\n",
        "    raw_notes_dir=settings.raw_notes_dir,\n",
        ")\n",
        "\n",
        "result = run_agentic_rag(graph, DRIFT_QUERY)\n",
        "\n",
        "print('Decision trace:')\n",
        "for step in result['decision_trace']:\n",
        "    print('-', step)\n",
        "\n",
        "print('\\nRewritten query:')\n",
        "print(result['rewritten_query'])\n",
        "\n",
        "print('\\nRetrieved chunks (score | doc_date | doc_title | chunk_id):')\n",
        "for chunk in result['retrieved_chunks']:\n",
        "    print(\n",
        "        f\"- {chunk['score']} | {chunk['doc_date']} | {chunk['doc_title']} | {chunk['chunk_id']}\"\n",
        "    )\n",
        "\n",
        "print('\\nGrade + retries:')\n",
        "print(f\"evidence_ok={result['evidence_ok']}, retry_count={result['retry_count']}, confidence={result['confidence']}\")\n",
        "\n",
        "print('\\nFinal answer payload:')\n",
        "print(json.dumps(result['final_answer'], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What changed vs baseline?\n",
        "\n",
        "The baseline pipeline in Notebook 03 is retrieve \u2192 generate with no explicit evidence checks, so it can drift to stale recommendations.\n",
        "\n",
        "Notebook 04 adds explicit control flow with LangGraph:\n",
        "1. rewrite query for recency intent\n",
        "2. retrieve chunks\n",
        "3. grade evidence for recency + relevance\n",
        "4. bounded retry (`MAX_RETRIES`) if evidence is weak\n",
        "5. generate grounded answer with citations + confidence (+ optional clarifying next step when confidence is low).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
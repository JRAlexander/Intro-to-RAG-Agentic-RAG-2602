{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline RAG (retrieve \u2192 generate) and timeframe drift failure\n",
        "\n",
        "This notebook loads the persisted Chroma index from Notebook 02 and runs a simple retrieve-then-generate baseline with OpenAI chat completions. It intentionally does **not** enforce recency so we can observe timeframe drift."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _find_project_root() -> Path:\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for base in (cwd, *cwd.parents):\n",
        "        if (base / \"src\" / \"config.py\").exists():\n",
        "            return base\n",
        "        nested = base / \"agentic-rag-second-brain\"\n",
        "        if (nested / \"src\" / \"config.py\").exists():\n",
        "            return nested\n",
        "    raise RuntimeError(\"Could not locate project root containing src/config.py\")\n",
        "\n",
        "PROJECT_ROOT = _find_project_root()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "from src.config import settings\n",
        "from src.rag_baseline import baseline_rag_answer\n",
        "from src.retrieval import load_persisted_index, retrieve_chunks\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", settings.openai_model)\n",
        "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", settings.embed_model)\n",
        "CHROMA_DIR = Path(os.getenv(\"CHROMA_DIR\", settings.chroma_dir)).resolve()\n",
        "TOP_K = int(os.getenv(\"TOP_K\", settings.top_k))\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", settings.temperature))\n",
        "MAX_CONTEXT_CHARS = int(os.getenv(\"MAX_CONTEXT_CHARS\", settings.max_context_chars))\n",
        "\n",
        "print(\"Config:\")\n",
        "print(f\"- PROJECT_ROOT: {PROJECT_ROOT}\")\n",
        "print(f\"- CHROMA_DIR: {CHROMA_DIR}\")\n",
        "print(f\"- EMBED_MODEL: {EMBED_MODEL}\")\n",
        "print(f\"- OPENAI_MODEL: {OPENAI_MODEL}\")\n",
        "print(f\"- TOP_K: {TOP_K}\")\n",
        "print(f\"- TEMPERATURE: {TEMPERATURE}\")\n",
        "print(f\"- MAX_CONTEXT_CHARS: {MAX_CONTEXT_CHARS}\")\n",
        "print(f\"- OPENAI_API_KEY set: {'yes' if OPENAI_API_KEY else 'no'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not OPENAI_API_KEY:\n",
        "    raise EnvironmentError(\n",
        "        \"OPENAI_API_KEY is required for Notebook 03. \"\n",
        "        \"Set it before running, for example: `export OPENAI_API_KEY='your-key'`.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "index = None\n",
        "try:\n",
        "    index = load_persisted_index(chroma_dir=CHROMA_DIR, embed_model=EMBED_MODEL)\n",
        "    print(f\"Loaded persisted index from: {CHROMA_DIR}\")\n",
        "except FileNotFoundError as err:\n",
        "    print(str(err))\n",
        "    print(\"Please run notebooks/02_indexing_chroma_llamaindex.ipynb first, then re-run this notebook.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "demo_queries = {\n",
        "    \"Q1 easy win\": \"What chunking overlap is currently recommended?\",\n",
        "    \"Q2 drift question\": \"What embedding model should we use?\",\n",
        "}\n",
        "\n",
        "demo_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if index is None:\n",
        "    print(\"Skipping retrieval + generation because persisted index is unavailable.\")\n",
        "else:\n",
        "    for label, query in demo_queries.items():\n",
        "        print(\"\n",
        "\" + \"=\" * 90)\n",
        "        print(f\"{label}: {query}\")\n",
        "\n",
        "        retrieved = retrieve_chunks(index=index, query=query, top_k=TOP_K)\n",
        "        retrieved_df = pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"score\": item[\"score\"],\n",
        "                    \"doc_date\": item[\"doc_date\"],\n",
        "                    \"doc_title\": item[\"doc_title\"],\n",
        "                    \"chunk_id\": item[\"chunk_id\"],\n",
        "                    \"snippet\": item[\"text\"][:220].replace(\"\n",
        "\", \" \"),\n",
        "                }\n",
        "                for item in retrieved\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        print(\"\n",
        "Retrieved chunks:\")\n",
        "        display(retrieved_df)\n",
        "\n",
        "        result = baseline_rag_answer(\n",
        "            index=index,\n",
        "            query=query,\n",
        "            top_k=TOP_K,\n",
        "            model=OPENAI_MODEL,\n",
        "            temperature=TEMPERATURE,\n",
        "            max_context_chars=MAX_CONTEXT_CHARS,\n",
        "        )\n",
        "\n",
        "        print(\"\n",
        "Answer:\")\n",
        "        print(result[\"answer\"])\n",
        "        if result.get(\"notes\"):\n",
        "            print(\"\n",
        "Notes:\")\n",
        "            print(result[\"notes\"])\n",
        "\n",
        "        citations_df = pd.DataFrame(result[\"citations\"])\n",
        "        print(\"\n",
        "Citations:\")\n",
        "        display(citations_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why baseline RAG can fail under timeframe drift\n",
        "\n",
        "This baseline pipeline retrieves semantically similar chunks and asks the model to answer from that context only. Because retrieval can surface chunks from different dates (older and newer recommendations), the model may blend or choose outdated guidance. In the next (agentic) notebook, we will add explicit temporal reasoning and conflict handling to improve consistency."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
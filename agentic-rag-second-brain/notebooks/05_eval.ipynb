{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ff78da",
   "metadata": {},
   "source": [
    "# 05 â€” Evaluation: Baseline vs Agentic\n",
    "\n",
    "Lightweight, repeatable evaluation over a golden question set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc795de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.eval import run_eval, build_comparison_report, top_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074820a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDEN_PATH = Path('../eval/golden_questions.jsonl')\n",
    "USE_LLM_EVAL = int(os.getenv('USE_LLM_EVAL', '0'))\n",
    "\n",
    "print(f'Golden set: {GOLDEN_PATH.resolve()}')\n",
    "print(f'USE_LLM_EVAL={USE_LLM_EVAL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e734b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df, agentic_df = run_eval(\n",
    "    golden_path=GOLDEN_PATH,\n",
    "    top_k=6,\n",
    "    max_retries=2,\n",
    "    use_llm_grader=False,\n",
    ")\n",
    "\n",
    "print(f'baseline rows: {len(baseline_df)}')\n",
    "print(f'agentic rows: {len(agentic_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = build_comparison_report(baseline_df, agentic_df)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87b8f6",
   "metadata": {},
   "source": [
    "## Drift-only comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02608bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df[report_df['scope'] == 'drift']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6350f5",
   "metadata": {},
   "source": [
    "## Top failures (3 examples each pipeline)\n",
    "\n",
    "Includes query, retrieved doc titles/dates, answer, citations, and failed deterministic checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_retrieved(chunks):\n",
    "    return [\n",
    "        {\n",
    "            'doc_title': c.get('doc_title', ''),\n",
    "            'doc_date': c.get('doc_date', ''),\n",
    "            'chunk_id': c.get('chunk_id', ''),\n",
    "        }\n",
    "        for c in (chunks or [])\n",
    "    ]\n",
    "\n",
    "for name, df in [('baseline', baseline_df), ('agentic', agentic_df)]:\n",
    "    print(f'\\n=== {name.upper()} ===')\n",
    "    failures = top_failures(df, n=3)\n",
    "    cols = ['question', 'answer', 'citations', 'checks_failed']\n",
    "    display(failures[cols])\n",
    "    for _, row in failures.iterrows():\n",
    "        print('---')\n",
    "        print('query:', row['question'])\n",
    "        print('retrieved (title/date/chunk):', compact_retrieved(row['retrieved_chunks']))\n",
    "        print('answer:', row['answer'])\n",
    "        print('citations:', row['citations'])\n",
    "        print('failed checks:', row['checks_failed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a34d3d",
   "metadata": {},
   "source": [
    "## Optional LLM-as-judge (disabled by default)\n",
    "\n",
    "Set `USE_LLM_EVAL=1` in `.env` to append `llm_judge_score` and `llm_judge_rationale` columns during `run_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ee3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LLM_EVAL == 1:\n",
    "    display(baseline_df[['id', 'question', 'llm_judge_score', 'llm_judge_rationale']].head())\n",
    "    display(agentic_df[['id', 'question', 'llm_judge_score', 'llm_judge_rationale']].head())\n",
    "else:\n",
    "    print('LLM judge disabled. Set USE_LLM_EVAL=1 to enable.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple RAG from CSV (No LangChain/LlamaIndex)\n",
        "\n",
        "This notebook demonstrates a minimal Retrieval-Augmented Generation (RAG) pipeline using:\n",
        "- **CSV** as the knowledge source\n",
        "- **OpenAI embeddings** (`text-embedding-3-small`) for retrieval\n",
        "- **OpenAI chat model** (`gpt-4o-mini`) for final answer generation\n",
        "\n",
        "The implementation avoids orchestration frameworks and uses only basic Python libraries (`pandas`, `numpy`, `openai`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, uncomment and run:\n",
        "# %pip install -q openai pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# Make sure your API key is available:\n",
        "# export OPENAI_API_KEY=\"your_key_here\"\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY before running this notebook.\"\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load CSV knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = \"../data/sample_knowledge.csv\"  # adjust if needed\n",
        "\n",
        "kb_df = pd.read_csv(csv_path)\n",
        "kb_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Build text chunks to embed\n",
        "\n",
        "For simplicity, each row is one chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kb_df[\"chunk_text\"] = (\n",
        "    \"Title: \" + kb_df[\"title\"].astype(str) + \"\\n\"\n",
        "    + \"Content: \" + kb_df[\"content\"].astype(str)\n",
        ")\n",
        "\n",
        "kb_df[[\"id\", \"chunk_text\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Create embeddings for all chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_model = \"text-embedding-3-small\"\n",
        "\n",
        "\n",
        "def get_embedding(text: str, model: str = embedding_model) -> np.ndarray:\n",
        "    response = client.embeddings.create(model=model, input=text)\n",
        "    return np.array(response.data[0].embedding, dtype=np.float32)\n",
        "\n",
        "kb_df[\"embedding\"] = kb_df[\"chunk_text\"].apply(get_embedding)\n",
        "print(f\"Created {len(kb_df)} embeddings. Vector size: {kb_df['embedding'].iloc[0].shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Retrieve top-k relevant chunks for a user query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "\n",
        "def retrieve(query: str, k: int = 3) -> pd.DataFrame:\n",
        "    query_emb = get_embedding(query)\n",
        "    scored = kb_df.copy()\n",
        "    scored[\"score\"] = scored[\"embedding\"].apply(lambda emb: cosine_similarity(query_emb, emb))\n",
        "    return scored.sort_values(\"score\", ascending=False).head(k)\n",
        "\n",
        "user_query = \"How long does international shipping take?\"\n",
        "retrieved = retrieve(user_query, k=3)\n",
        "retrieved[[\"id\", \"title\", \"score\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Build augmented prompt and generate answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generation_model = \"gpt-4o-mini\"\n",
        "\n",
        "context = \"\\n\\n---\\n\\n\".join(retrieved[\"chunk_text\"].tolist())\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a helpful assistant. Answer only from the provided context. \"\n",
        "    \"If the answer is not in context, say you don't know.\"\n",
        ")\n",
        "\n",
        "user_prompt = f\"\"\"\n",
        "Question: {user_query}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=generation_model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Optional: wrap into one function\n",
        "\n",
        "Use this to ask multiple questions after embeddings are built once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_rag(question: str, k: int = 3) -> str:\n",
        "    top_docs = retrieve(question, k=k)\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(top_docs[\"chunk_text\"].tolist())\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{context_text}\n",
        "\"\"\"\n",
        "\n",
        "    answer = client.chat.completions.create(\n",
        "        model=generation_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return answer.choices[0].message.content\n",
        "\n",
        "ask_rag(\"When can I get a refund?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}